apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: cuegrowth-alerts
  namespace: cuegrowth
  labels:
    prometheus: kube-prometheus
    release: prometheus
spec:
  groups:
  - name: cuegrowth.rules
    interval: 30s
    rules:
    # Alert when API error rate is high
    - alert: HighAPIErrorRate
      expr: |
        sum(rate(api_requests_total{status=~"5.."}[5m])) 
        / 
        sum(rate(api_requests_total[5m])) > 0.05
      for: 5m
      labels:
        severity: warning
        component: api-gateway
      annotations:
        summary: "High API error rate detected"
        description: "API error rate is {{ $value | humanizePercentage }} over the last 5 minutes"
    
    # Alert when queue backlog is growing
    - alert: HighQueueBacklog
      expr: worker_queue_backlog > 1000
      for: 10m
      labels:
        severity: warning
        component: queue
      annotations:
        summary: "High queue backlog"
        description: "Queue has {{ $value }} pending messages"
    
    # Alert when worker processing is slow
    - alert: SlowWorkerProcessing
      expr: rate(worker_tasks_processed_total[5m]) < 1
      for: 10m
      labels:
        severity: warning
        component: worker
      annotations:
        summary: "Worker processing rate is low"
        description: "Workers are processing less than 1 task per second"
    
    # Alert when API latency is high
    - alert: HighAPILatency
      expr: |
        histogram_quantile(0.95, 
          sum(rate(api_request_duration_seconds_bucket[5m])) by (le, endpoint)
        ) > 0.5
      for: 5m
      labels:
        severity: warning
        component: api-gateway
      annotations:
        summary: "High API latency"
        description: "API p95 latency is {{ $value }}s for endpoint {{ $labels.endpoint }}"
    
    # Alert when pods are restarting frequently
    - alert: FrequentPodRestarts
      expr: rate(kube_pod_container_status_restarts_total{namespace="cuegrowth"}[1h]) > 0.1
      for: 5m
      labels:
        severity: critical
        component: kubernetes
      annotations:
        summary: "Pod {{ $labels.pod }} is restarting frequently"
        description: "Pod has restarted {{ $value }} times in the last hour"
    
    # Alert when Valkey is down
    - alert: ValkeyDown
      expr: up{job="cuegrowth-nats", app="valkey"} == 0
      for: 2m
      labels:
        severity: critical
        component: valkey
      annotations:
        summary: "Valkey is down"
        description: "Valkey instance {{ $labels.instance }} is unreachable"
    
    # Alert when NATS is down
    - alert: NATSDown
      expr: up{job="cuegrowth-nats", app="nats"} == 0
      for: 2m
      labels:
        severity: critical
        component: nats
      annotations:
        summary: "NATS is down"
        description: "NATS instance {{ $labels.instance }} is unreachable"
    
    # Alert on high memory usage
    - alert: HighMemoryUsage
      expr: |
        sum(container_memory_working_set_bytes{namespace="cuegrowth"}) by (pod) 
        / 
        sum(container_spec_memory_limit_bytes{namespace="cuegrowth"}) by (pod) > 0.9
      for: 5m
      labels:
        severity: warning
        component: kubernetes
      annotations:
        summary: "Pod {{ $labels.pod }} memory usage is high"
        description: "Memory usage is at {{ $value | humanizePercentage }}"
    
    # Alert on high CPU usage
    - alert: HighCPUUsage
      expr: |
        sum(rate(container_cpu_usage_seconds_total{namespace="cuegrowth"}[5m])) by (pod) 
        / 
        sum(container_spec_cpu_quota{namespace="cuegrowth"}/container_spec_cpu_period{namespace="cuegrowth"}) by (pod) > 0.9
      for: 10m
      labels:
        severity: warning
        component: kubernetes
      annotations:
        summary: "Pod {{ $labels.pod }} CPU usage is high"
        description: "CPU usage is at {{ $value | humanizePercentage }}"
        